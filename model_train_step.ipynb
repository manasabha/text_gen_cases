{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#import Keras library\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "#import spacy, and spacy french model\n",
    "# spacy is used to work on text\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "#import other libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "#define parameters used in the tutorial\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/all_cases.txt'# data directory containing raw texts\n",
    "save_dir = '../data/'\n",
    "save_model_dir = '../data/models/'\n",
    "\n",
    "file_list = []\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "sequences_step = 1 #step to create sequences\n",
    "given_length = 30 # length of given words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_list(path):\n",
    "\n",
    "    print('Starting wordlist')\n",
    "    word_list = []\n",
    "    count = 0\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in tqdm(f):\n",
    "            if count > 5001:\n",
    "                break\n",
    "            if count % 5000 == 0:\n",
    "                print(\"Read {} sentences\".format(count))\n",
    "            words = map(lambda x:x.lower(), line.strip().split())\n",
    "            word_list = word_list + words\n",
    "            count = count +1\n",
    "\n",
    "    print ('Done with wordlist')\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def create_vocab(out_path, word_list):\n",
    "\n",
    "    print('Starting create vocab')\n",
    "    # count the number of words\n",
    "    word_counts = collections.Counter(word_list)\n",
    "\n",
    "    # Mapping from index to word : that's the vocabulary\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "    # Mapping from word to index\n",
    "    voca = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    words_dict = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "    # size of the vocabulary\n",
    "    voca_size = len(words_dict)\n",
    "    print(\"vocab size: \", voca_size)\n",
    "\n",
    "    # save the words and vocabulary\n",
    "    with open(os.path.join(vocab_file), 'wb') as f:\n",
    "        cPickle.dump((words_dict, voca, vocabulary_inv), f)\n",
    "\n",
    "    print ('Done with create vocab')\n",
    "    return voca_size, voca, words_dict\n",
    "\n",
    "\n",
    "def create_given_and_next_word_training_data(word_list):\n",
    "\n",
    "    print('Starting sequence and Y words')\n",
    "\n",
    "    given_words_list = []\n",
    "    next_word_for_given = []\n",
    "\n",
    "    for i in range(0, len(word_list) - given_length, sequences_step):\n",
    "        given_words_list.append(word_list[i: i + given_length])\n",
    "        next_word_for_given.append(word_list[i + given_length])\n",
    "\n",
    "    print ('Done with sequence and Y words')\n",
    "    return given_words_list, next_word_for_given\n",
    "\n",
    "\n",
    "def create_X_and_Y_for_train(vocab, vocab_size, word_list):\n",
    "\n",
    "    print('Starting matrix creation')\n",
    "\n",
    "    sequences, next_words = create_given_and_next_word_training_data(word_list)\n",
    "\n",
    "    X = np.zeros((len(sequences), given_length, vocab_size), dtype=np.bool)\n",
    "    y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sequences):\n",
    "        for t, word in enumerate(sentence):\n",
    "            X[i, t, vocab[word]] = 1\n",
    "        y[i, vocab[next_words[i]]] = 1\n",
    "\n",
    "    print ('Done with matrix creation')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "\n",
    "    print('Starting model creation')\n",
    "\n",
    "    lstm_cells = 256  # no. of LSTM cells\n",
    "    seq_length = 30  # sequence length\n",
    "    learning_rate = 0.001  # learning rate\n",
    "\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(lstm_cells, activation=\"relu\"), input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks = [EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    print(\"model built!\")\n",
    "    print ('Done with model creation')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "793it [00:00, 3897.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting wordlist\n",
      "Read 0 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [00:09, 263.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 5000 sentences\n",
      "Done with wordlist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = create_word_list(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting create vocab\n",
      "vocab size:  32562\n",
      "Done with create vocab\n"
     ]
    }
   ],
   "source": [
    "vocab_size, vocab, words = create_vocab(save_dir, word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting matrix creation\n",
      "Starting sequence and Y words\n",
      "Done with sequence and Y words\n",
      "Done with matrix creation\n"
     ]
    }
   ],
   "source": [
    "X, y = create_X_and_Y_for_train(vocab, vocab_size, word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model creation\n",
      "Build LSTM model.\n",
      "model built!\n",
      "Done with model creation\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               67213312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32562)             16704306  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32562)             0         \n",
      "=================================================================\n",
      "Total params: 83,917,618\n",
      "Trainable params: 83,917,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = bidirectional_lstm_model(given_length, vocab_size)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 512  # minibatch size\n",
    "num_epochs = 10  # number of epochs\n",
    "\n",
    "callbacks = [EarlyStopping(patience=4, monitor='val_loss'),\n",
    "             ModelCheckpoint(filepath=save_model_dir + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5', \\\n",
    "                             monitor='val_loss', verbose=0, mode='auto', period=2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 310405 samples, validate on 133031 samples\n",
      "Epoch 1/10\n",
      "   512/310405 [..............................] - ETA: 9:01:57 - loss: 10.3909 - categorical_accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(X, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.3)\n",
    "\n",
    "# save the model\n",
    "model.save(save_dir + \"/\" + 'my_model_generate_sentences.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
